{
  "modules": [
    
    {
      "name": "kfold_label",
    "corpus": [
              "create kfold",
              "create kfolds",
              "make kfold",
              "create kfold labels",
              "create subset folds",
              "make subset fold",
              "label kfold"
              ],
      "info": {
              "module":"make_folds",
              "action":"create subset",
              "topic":"subset generation",
              "subtopic":"kfold cross validation",
              "input_format":"pd.DataFrame",
              "description":"K-fold cross-validation is a technique used in machine learning to evaluate the performance of a model. It involves dividing the dataset into k equal-sized subsets, or folds. The model is then trained on k-1 folds and tested on the remaining fold. This process is repeated k times, with each fold being used as the test set once. The results are averaged across the k iterations to provide an estimate of the model's performance. K-fold cross-validation helps to reduce the risk of overfitting and provides a more accurate estimate of the model's generalization performance. It is commonly used in machine learning to tune hyperparameters, select models, and compare different algorithms.",
              "arg_compat":"splits shuffle rs"
              }
    },

    {
      "name": "skfold_label",
    "corpus": [
              "stratified kfold",
              "stratified kfolds",
              "create stratified kfold",
              "make stratified kfold",
              "generate stratified kfold",
              "label statified kfold"
              ],
      "info": {
              "module":"make_folds",
              "action":"create subset",
              "topic":"subset generation",
              "subtopic":"stratified kfold cross validation",
              "input_format":"pd.DataFrame",
              "description":"Stratified k-fold cross-validation is a variation of k-fold cross-validation that ensures that each fold is representative of the overall distribution of the target variable. This is particularly useful when dealing with imbalanced datasets, where one class may be significantly underrepresented. In stratified k-fold cross-validation, the dataset is divided into k folds, but the division is done in such a way that each fold contains approximately the same proportion of samples from each class as the original dataset. This ensures that each fold is representative of the overall distribution of the target variable, and reduces the risk of bias in the evaluation of the model's performance. Stratified k-fold cross-validation is commonly used in classification tasks where the goal is to predict the class label of a sample based on its features.",
              "arg_compat":"splits shuffle rs"
              }
    },

    {
      "name": "tts_label",
    "corpus": [
              "train test split label",
              "train test split labels",
              "train test splitting labels",
              "create tts label",
              "make tts label",
              "make train test split label",
              "train-test-split label",
              "create train-test-split label",
              "label tts",
              "tts labels",
              "create tts labels"
              ],
      "info": {
              "module":"make_folds",
              "action":"create subset",
              "topic":"subset generation",
              "subtopic":"train test split",
              "input_format":"pd.DataFrame",
              "description":"Train test splitting is a technique used in machine learning to evaluate the performance of a model. It involves dividing the available dataset into two subsets: the training set and the testing set. The training set is used to train the model, while the testing set is used to evaluate its performance. The idea behind train test splitting is to assess how well the model generalizes to new, unseen data. By evaluating the model on a separate testing set, we can get an estimate of its performance on new data that it has not seen before. The size of the training and testing sets can vary depending on the size of the dataset, but a common practice is to use 70-80 of the data for training and the remaining 20-30 for testing.",
              "arg_compat":"test_size shuffle rs"
              }
    }

  ]
}